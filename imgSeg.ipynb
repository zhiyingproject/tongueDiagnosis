{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imgSeg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zhiyingproject/tongueDiagnosis/blob/main/imgSeg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECDyxU8NSj1l",
        "outputId": "80316d71-70f2-4d13-9145-ab1779564c82"
      },
      "source": [
        "# This notebook is to train a model for locating tongues from people's faces. It is for tongue diagnosis using traditional Chinese Medical Theory (TCM)\n",
        "!pip uninstall keras-nightly\n",
        "!pip install h5py==2.10.0\n",
        "!pip install --upgrade tensorflow==1.15.0\n",
        "!pip install --upgrade tensorflow-gpu==1.15\n",
        "# !pip install --upgrade keras==2.2.4\n",
        "!python -m pip install -U scikit-image==0.16.2\n",
        "!pip uninstall keras \n",
        "!pip install keras==2.2.4\n",
        "!pip install wcmatch\n",
        "# %tensorflow_version 1.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found existing installation: keras-nightly 2.5.0.dev2021032900\n",
            "Uninstalling keras-nightly-2.5.0.dev2021032900:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/*\n",
            "    /usr/local/lib/python3.7/dist-packages/keras_nightly-2.5.0.dev2021032900.dist-info/*\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/applications/resnet50.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/engine/network.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/engine/topology.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/initializers.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/layers/experimental/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/layers/experimental/preprocessing/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/objectives.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/optimizers/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/optimizers/schedules/__init__.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/utils/test_utils.py\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled keras-nightly-2.5.0.dev2021032900\n",
            "Collecting h5py==2.10.0\n",
            "  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 7.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.19.5)\n",
            "Installing collected packages: h5py\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.5.0 requires keras-nightly~=2.5.0.dev, which is not installed.\n",
            "tensorflow 2.5.0 requires h5py~=3.1.0, but you have h5py 2.10.0 which is incompatible.\u001b[0m\n",
            "Successfully installed h5py-2.10.0\n",
            "Collecting tensorflow==1.15.0\n",
            "  Downloading tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3 MB 12 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.19.5)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.36.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.17.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading gast-0.2.2.tar.gz (10 kB)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "  Downloading tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503 kB)\n",
            "\u001b[K     |████████████████████████████████| 503 kB 57.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.34.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "  Downloading tensorboard-1.15.0-py3-none-any.whl (3.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8 MB 63.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Collecting keras-applications>=1.0.8\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (57.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.5.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-py3-none-any.whl size=7553 sha256=d8dfd45c4fc248fa3b0898ab72675a96887c990f2189d08e6bbae32fadec4104\n",
            "  Stored in directory: /root/.cache/pip/wheels/21/7f/02/420f32a803f7d0967b48dd823da3f558c5166991bfd204eef3\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, keras-applications, gast, tensorflow\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Attempting uninstall: gast\n",
            "    Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.5.0\n",
            "    Uninstalling tensorflow-2.5.0:\n",
            "      Successfully uninstalled tensorflow-2.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-probability 0.13.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "kapre 0.3.5 requires tensorflow>=2.0.0, but you have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n",
            "Collecting tensorflow-gpu==1.15\n",
            "  Downloading tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5 MB 7.6 kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.2.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.34.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.19.5)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.15.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.36.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.15.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.17.3)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (3.3.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.12.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (1.12.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==1.15) (0.8.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==1.15) (2.10.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (57.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.3.4)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (4.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow-gpu==1.15) (3.5.0)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.15.0\n",
            "Requirement already satisfied: scikit-image==0.16.2 in /usr/local/lib/python3.7/dist-packages (0.16.2)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.16.2) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.16.2) (1.1.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.16.2) (2.5.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.16.2) (2.4.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.16.2) (7.1.2)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image==0.16.2) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from imageio>=2.3.0->scikit-image==0.16.2) (1.19.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from cycler>=0.10->matplotlib!=3.0.0,>=2.0.0->scikit-image==0.16.2) (1.15.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->scikit-image==0.16.2) (4.4.2)\n",
            "Found existing installation: Keras 2.4.3\n",
            "Uninstalling Keras-2.4.3:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/Keras-2.4.3.dist-info/*\n",
            "    /usr/local/lib/python3.7/dist-packages/docs/*\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/applications/resnet50.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/engine/network.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/engine/topology.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/initializers.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/layers/experimental/*\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/objectives.py\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/optimizers/*\n",
            "    /usr/local/lib/python3.7/dist-packages/keras/utils/test_utils.py\n",
            "  Would not remove (might be manually added):\n",
            "    /usr/local/lib/python3.7/dist-packages/docs/md_autogen.py\n",
            "    /usr/local/lib/python3.7/dist-packages/docs/update_docs.py\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled Keras-2.4.3\n",
            "Collecting keras==2.2.4\n",
            "  Downloading Keras-2.2.4-py2.py3-none-any.whl (312 kB)\n",
            "\u001b[K     |████████████████████████████████| 312 kB 7.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (2.10.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.19.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from keras==2.2.4) (1.1.2)\n",
            "Installing collected packages: keras\n",
            "Successfully installed keras-2.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "YQgfikcgWuLW",
        "outputId": "34fd76ba-064c-418c-dc0e-04caa4e4e86f"
      },
      "source": [
        "# Restarting the kernel to enable tensorflow v1.14\n",
        "from IPython.core.display import HTML\n",
        "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<script>Jupyter.notebook.kernel.restart()</script>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tlG8iofWvg7",
        "outputId": "5b0a6eac-6817-419a-8d21-baa688531c05"
      },
      "source": [
        "# Install Mask RCNN\n",
        "!git clone https://github.com/matterport/Mask_RCNN.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Mask_RCNN'...\n",
            "remote: Enumerating objects: 956, done.\u001b[K\n",
            "remote: Total 956 (delta 0), reused 0 (delta 0), pack-reused 956\u001b[K\n",
            "Receiving objects: 100% (956/956), 125.23 MiB | 30.50 MiB/s, done.\n",
            "Resolving deltas: 100% (562/562), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIMB9aLXW2jk",
        "outputId": "e8864ca1-e418-41b9-b586-40a55acba8c8"
      },
      "source": [
        "cd Mask_RCNN/"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/Mask_RCNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-eVzAQ8W6LG",
        "outputId": "56de8ef2-32d5-404b-8b7a-269d3a5d31fb"
      },
      "source": [
        "# Setup libraries for Mas RCNN\n",
        "!python setup.py install\n",
        "!pip show mask-rcnn"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:root:Fail load requirements file, so using default ones.\n",
            "/usr/local/lib/python3.7/dist-packages/setuptools/dist.py:700: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n",
            "  % (opt, underscore_opt))\n",
            "/usr/local/lib/python3.7/dist-packages/setuptools/dist.py:700: UserWarning: Usage of dash-separated 'license-file' will not be supported in future versions. Please use the underscore name 'license_file' instead\n",
            "  % (opt, underscore_opt))\n",
            "/usr/local/lib/python3.7/dist-packages/setuptools/dist.py:700: UserWarning: Usage of dash-separated 'requirements-file' will not be supported in future versions. Please use the underscore name 'requirements_file' instead\n",
            "  % (opt, underscore_opt))\n",
            "running install\n",
            "running bdist_egg\n",
            "running egg_info\n",
            "creating mask_rcnn.egg-info\n",
            "writing mask_rcnn.egg-info/PKG-INFO\n",
            "writing dependency_links to mask_rcnn.egg-info/dependency_links.txt\n",
            "writing top-level names to mask_rcnn.egg-info/top_level.txt\n",
            "writing manifest file 'mask_rcnn.egg-info/SOURCES.txt'\n",
            "reading manifest template 'MANIFEST.in'\n",
            "adding license file 'LICENSE'\n",
            "writing manifest file 'mask_rcnn.egg-info/SOURCES.txt'\n",
            "installing library code to build/bdist.linux-x86_64/egg\n",
            "running install_lib\n",
            "running build_py\n",
            "creating build\n",
            "creating build/lib\n",
            "creating build/lib/mrcnn\n",
            "copying mrcnn/__init__.py -> build/lib/mrcnn\n",
            "copying mrcnn/visualize.py -> build/lib/mrcnn\n",
            "copying mrcnn/config.py -> build/lib/mrcnn\n",
            "copying mrcnn/model.py -> build/lib/mrcnn\n",
            "copying mrcnn/parallel_model.py -> build/lib/mrcnn\n",
            "copying mrcnn/utils.py -> build/lib/mrcnn\n",
            "creating build/bdist.linux-x86_64\n",
            "creating build/bdist.linux-x86_64/egg\n",
            "creating build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/__init__.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/visualize.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/config.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/parallel_model.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "copying build/lib/mrcnn/utils.py -> build/bdist.linux-x86_64/egg/mrcnn\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/__init__.py to __init__.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/visualize.py to visualize.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/config.py to config.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/model.py to model.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/parallel_model.py to parallel_model.cpython-37.pyc\n",
            "byte-compiling build/bdist.linux-x86_64/egg/mrcnn/utils.py to utils.cpython-37.pyc\n",
            "creating build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/PKG-INFO -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/SOURCES.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/dependency_links.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "copying mask_rcnn.egg-info/top_level.txt -> build/bdist.linux-x86_64/egg/EGG-INFO\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "creating dist\n",
            "creating 'dist/mask_rcnn-2.1-py3.7.egg' and adding 'build/bdist.linux-x86_64/egg' to it\n",
            "removing 'build/bdist.linux-x86_64/egg' (and everything under it)\n",
            "Processing mask_rcnn-2.1-py3.7.egg\n",
            "Copying mask_rcnn-2.1-py3.7.egg to /usr/local/lib/python3.7/dist-packages\n",
            "Adding mask-rcnn 2.1 to easy-install.pth file\n",
            "\n",
            "Installed /usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg\n",
            "Processing dependencies for mask-rcnn==2.1\n",
            "Finished processing dependencies for mask-rcnn==2.1\n",
            "Name: mask-rcnn\n",
            "Version: 2.1\n",
            "Summary: Mask R-CNN for object detection and instance segmentation\n",
            "Home-page: https://github.com/matterport/Mask_RCNN\n",
            "Author: Matterport\n",
            "Author-email: waleed.abdulla@gmail.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.7/dist-packages/mask_rcnn-2.1-py3.7.egg\n",
            "Requires: \n",
            "Required-by: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hAIIagFGXrnJ",
        "outputId": "5d762e3b-d772-4310-a7f5-2d08ce49bcba"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NWmN6kDRZN3M",
        "outputId": "83e80d7a-1de9-4c2d-d905-1c2317c18fa2"
      },
      "source": [
        "\n",
        "import pathlib\n",
        "import shutil\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "import sys\n",
        "from mrcnn.utils import Dataset, extract_bboxes, compute_ap\n",
        "from mrcnn.config import Config\n",
        "from mrcnn.visualize import display_instances\n",
        "from numpy import zeros, asarray\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from mrcnn.model import MaskRCNN, load_image_gt, mold_image\n",
        "from numpy import zeros, asarray, expand_dims, mean\n",
        "import math"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ja0fZ4TUYRq2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42efd29b-34cc-4380-b7de-13e1ec37b2fa"
      },
      "source": [
        "\n",
        "from wcmatch.pathlib import Path\n",
        "ds_path = '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining'\n",
        "num_files = len(list(Path(ds_path).rglob(['*.jpg', '*.JPG', '*.jpeg'])))\n",
        "training_threshold = int(num_files * 0.9)\n",
        "print(training_threshold, num_files - training_threshold)\n",
        "model_path = '/content/drive/MyDrive/backup/tongueDiagnosis/models'"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "485 54\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_omxXKCBZHOp"
      },
      "source": [
        "class tongueDataset(Dataset):\n",
        "    def load_dataset(self, dataset_dir, is_train=True):\n",
        "        self.add_class(\"dataset\", 1, \"tongue\")\n",
        "\n",
        "        images_dir = dataset_dir\n",
        "        annotations_dir = f'{dataset_dir}/annots'\n",
        "        image_files = Path(dataset_dir).rglob(['*.jpg','*.JPG','*.jpeg'])\n",
        "        for ifile, filename in enumerate(list(image_files)):\n",
        "            image_id = filename.stem\n",
        "            if is_train and ifile >= training_threshold:\n",
        "                continue\n",
        "            if not is_train and ifile < training_threshold:\n",
        "                continue\n",
        "            img_path = filename\n",
        "            ann_path = f'{annotations_dir}/{filename.stem}.xml'\n",
        "            self.add_image('dataset', image_id=image_id, path=img_path, annotation=ann_path)\n",
        "\n",
        "    def extract_boxes(self, filename):\n",
        "        tree = ET.parse(filename)\n",
        "        root = tree.getroot()\n",
        "        boxes = list()\n",
        "        for box in root.findall('.//bndbox'):\n",
        "            xmin = int(box.find('xmin').text)\n",
        "            ymin = int(box.find('ymin').text)\n",
        "            xmax = int(box.find('xmax').text)\n",
        "            ymax = int(box.find('ymax').text)\n",
        "            coors = [xmin, ymin, xmax, ymax]\n",
        "            boxes.append(coors)\n",
        "        width = int(root.find('.//size/width').text)\n",
        "        height = int(root.find('.//size/height').text)\n",
        "        return boxes, width, height\n",
        "\n",
        "    def load_mask(self, image_id):\n",
        "        info = self.image_info[image_id]\n",
        "        path = info['annotation']\n",
        "        boxes, w, h = self.extract_boxes(path)\n",
        "        masks = zeros([h, w, len(boxes)], dtype='uint8')\n",
        "\n",
        "        class_ids = []\n",
        "        for i in range(len(boxes)):\n",
        "            box = boxes[i]\n",
        "            row_s, row_e = box[1], box[3]\n",
        "            col_s, col_e = box[0], box[2]\n",
        "            masks[row_s:row_e, col_s:col_e, i] = 1\n",
        "            class_ids.append(self.class_names.index('tongue'))\n",
        "        return masks, asarray(class_ids, dtype='int32')\n",
        "\n",
        "    def image_reference(self, image_id):\n",
        "        info = self.image_info[image_id]\n",
        "        return info['path']"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqdCwjghZbaZ"
      },
      "source": [
        "class tongueConfig(Config):\n",
        "    NAME = 'tongue_cfg'\n",
        "    NUM_CLASSES = 1 + 1\n",
        "    STEPS_PER_EPOCH = training_threshold\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFx1QEARZpBi",
        "outputId": "bad5199e-330d-443a-ae35-bbe281b6e0e0"
      },
      "source": [
        "training_ds = tongueDataset()\n",
        "training_ds.load_dataset(ds_path, is_train=True)\n",
        "training_ds.prepare()\n",
        "print('Train: %d' % len(training_ds.image_ids))\n",
        "\n",
        "test_ds = tongueDataset()\n",
        "test_ds.load_dataset(ds_path, is_train=False)\n",
        "test_ds.prepare()\n",
        "print('Test: %d' % len(test_ds.image_ids))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 485\n",
            "Test: 54\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vsq59vwvZwTJ",
        "outputId": "c9a72a13-3822-4713-cc06-9ed6463b2684"
      },
      "source": [
        "config = tongueConfig()\n",
        "config.display()\n",
        "\n",
        "model = MaskRCNN(mode='training', model_dir=model_path, config=config)\n",
        "model.load_weights('/content/drive/MyDrive/backup/tongueDiagnosis/models/mask_rcnn_coco.h5', \n",
        "                   by_name=True,\n",
        "                   exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\",  \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
        "model.train(training_ds, test_ds, learning_rate=config.LEARNING_RATE, epochs=5, layers='heads')\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Configurations:\n",
            "BACKBONE                       resnet101\n",
            "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
            "BATCH_SIZE                     1\n",
            "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
            "COMPUTE_BACKBONE_SHAPE         None\n",
            "DETECTION_MAX_INSTANCES        100\n",
            "DETECTION_MIN_CONFIDENCE       0.7\n",
            "DETECTION_NMS_THRESHOLD        0.3\n",
            "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
            "GPU_COUNT                      1\n",
            "GRADIENT_CLIP_NORM             5.0\n",
            "IMAGES_PER_GPU                 1\n",
            "IMAGE_CHANNEL_COUNT            3\n",
            "IMAGE_MAX_DIM                  1024\n",
            "IMAGE_META_SIZE                14\n",
            "IMAGE_MIN_DIM                  800\n",
            "IMAGE_MIN_SCALE                0\n",
            "IMAGE_RESIZE_MODE              square\n",
            "IMAGE_SHAPE                    [1024 1024    3]\n",
            "LEARNING_MOMENTUM              0.9\n",
            "LEARNING_RATE                  0.001\n",
            "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
            "MASK_POOL_SIZE                 14\n",
            "MASK_SHAPE                     [28, 28]\n",
            "MAX_GT_INSTANCES               100\n",
            "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
            "MINI_MASK_SHAPE                (56, 56)\n",
            "NAME                           tongue_cfg\n",
            "NUM_CLASSES                    2\n",
            "POOL_SIZE                      7\n",
            "POST_NMS_ROIS_INFERENCE        1000\n",
            "POST_NMS_ROIS_TRAINING         2000\n",
            "PRE_NMS_LIMIT                  6000\n",
            "ROI_POSITIVE_RATIO             0.33\n",
            "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
            "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
            "RPN_ANCHOR_STRIDE              1\n",
            "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
            "RPN_NMS_THRESHOLD              0.7\n",
            "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
            "STEPS_PER_EPOCH                276\n",
            "TOP_DOWN_PYRAMID_SIZE          256\n",
            "TRAIN_BN                       False\n",
            "TRAIN_ROIS_PER_IMAGE           200\n",
            "USE_MINI_MASK                  True\n",
            "USE_RPN_ROIS                   True\n",
            "VALIDATION_STEPS               50\n",
            "WEIGHT_DECAY                   0.0001\n",
            "\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1919: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:2018: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:553: The name tf.random_shuffle is deprecated. Please use tf.random.shuffle instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/utils.py:202: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /content/Mask_RCNN/mrcnn/model.py:600: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "box_ind is deprecated, use box_indices instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:186: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:199: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:206: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "\n",
            "Starting at epoch 0. LR=0.001\n",
            "\n",
            "Checkpoint Path: /content/drive/MyDrive/backup/tongueDiagnosis/models/tongue_cfg20210806T2129/mask_rcnn_tongue_cfg_{epoch:04d}.h5\n",
            "Selecting layers to train\n",
            "fpn_c5p5               (Conv2D)\n",
            "fpn_c4p4               (Conv2D)\n",
            "fpn_c3p3               (Conv2D)\n",
            "fpn_c2p2               (Conv2D)\n",
            "fpn_p5                 (Conv2D)\n",
            "fpn_p2                 (Conv2D)\n",
            "fpn_p3                 (Conv2D)\n",
            "fpn_p4                 (Conv2D)\n",
            "In model:  rpn_model\n",
            "    rpn_conv_shared        (Conv2D)\n",
            "    rpn_class_raw          (Conv2D)\n",
            "    rpn_bbox_pred          (Conv2D)\n",
            "mrcnn_mask_conv1       (TimeDistributed)\n",
            "mrcnn_mask_bn1         (TimeDistributed)\n",
            "mrcnn_mask_conv2       (TimeDistributed)\n",
            "mrcnn_mask_bn2         (TimeDistributed)\n",
            "mrcnn_class_conv1      (TimeDistributed)\n",
            "mrcnn_class_bn1        (TimeDistributed)\n",
            "mrcnn_mask_conv3       (TimeDistributed)\n",
            "mrcnn_mask_bn3         (TimeDistributed)\n",
            "mrcnn_class_conv2      (TimeDistributed)\n",
            "mrcnn_class_bn2        (TimeDistributed)\n",
            "mrcnn_mask_conv4       (TimeDistributed)\n",
            "mrcnn_mask_bn4         (TimeDistributed)\n",
            "mrcnn_bbox_fc          (TimeDistributed)\n",
            "mrcnn_mask_deconv      (TimeDistributed)\n",
            "mrcnn_class_logits     (TimeDistributed)\n",
            "mrcnn_mask             (TimeDistributed)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/framework/indexed_slices.py:424: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:973: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
            "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 1/5\n",
            "276/276 [==============================] - 181s 656ms/step - loss: 0.8719 - rpn_class_loss: 0.0031 - rpn_bbox_loss: 0.1898 - mrcnn_class_loss: 0.0244 - mrcnn_bbox_loss: 0.3919 - mrcnn_mask_loss: 0.2627 - val_loss: 0.6557 - val_rpn_class_loss: 0.0034 - val_rpn_bbox_loss: 0.2009 - val_mrcnn_class_loss: 0.0110 - val_mrcnn_bbox_loss: 0.2284 - val_mrcnn_mask_loss: 0.2119\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/callbacks.py:995: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "Epoch 2/5\n",
            "276/276 [==============================] - 147s 532ms/step - loss: 0.5764 - rpn_class_loss: 0.0031 - rpn_bbox_loss: 0.1779 - mrcnn_class_loss: 0.0132 - mrcnn_bbox_loss: 0.1798 - mrcnn_mask_loss: 0.2024 - val_loss: 0.5530 - val_rpn_class_loss: 0.0051 - val_rpn_bbox_loss: 0.1826 - val_mrcnn_class_loss: 0.0076 - val_mrcnn_bbox_loss: 0.1720 - val_mrcnn_mask_loss: 0.1857\n",
            "Epoch 3/5\n",
            "276/276 [==============================] - 146s 528ms/step - loss: 0.4449 - rpn_class_loss: 0.0034 - rpn_bbox_loss: 0.1540 - mrcnn_class_loss: 0.0104 - mrcnn_bbox_loss: 0.1167 - mrcnn_mask_loss: 0.1604 - val_loss: 0.5142 - val_rpn_class_loss: 0.0032 - val_rpn_bbox_loss: 0.1545 - val_mrcnn_class_loss: 0.0113 - val_mrcnn_bbox_loss: 0.1557 - val_mrcnn_mask_loss: 0.1895\n",
            "Epoch 4/5\n",
            "276/276 [==============================] - 146s 531ms/step - loss: 0.4095 - rpn_class_loss: 0.0027 - rpn_bbox_loss: 0.1338 - mrcnn_class_loss: 0.0104 - mrcnn_bbox_loss: 0.1066 - mrcnn_mask_loss: 0.1560 - val_loss: 0.4624 - val_rpn_class_loss: 0.0017 - val_rpn_bbox_loss: 0.1956 - val_mrcnn_class_loss: 0.0057 - val_mrcnn_bbox_loss: 0.0789 - val_mrcnn_mask_loss: 0.1805\n",
            "Epoch 5/5\n",
            "276/276 [==============================] - 147s 531ms/step - loss: 0.3817 - rpn_class_loss: 0.0029 - rpn_bbox_loss: 0.1254 - mrcnn_class_loss: 0.0088 - mrcnn_bbox_loss: 0.0954 - mrcnn_mask_loss: 0.1491 - val_loss: 0.4711 - val_rpn_class_loss: 0.0047 - val_rpn_bbox_loss: 0.1835 - val_mrcnn_class_loss: 0.0064 - val_mrcnn_bbox_loss: 0.1126 - val_mrcnn_mask_loss: 0.1638\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bi9LMjVubRqq"
      },
      "source": [
        "class predictionConfig(Config):\n",
        "    NAME = 'tongue_cfg'\n",
        "    NUM_CLASSES = 1 + 1\n",
        "    GPU_COUNT = 1\n",
        "    IMAGES_PER_GPU = 1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtFexi1UbTtt"
      },
      "source": [
        "def evaluate_model(ds, model, cfg):\n",
        "    APs = list()\n",
        "    for i, img_id in enumerate(ds.image_ids):\n",
        "        print(ds.image_info[i])\n",
        "        image, image_meta, gt_class_id, gt_bbox, gt_mask = load_image_gt(ds, cfg, img_id, use_mini_mask=False)\n",
        "        scaled_image = mold_image(image, cfg)\n",
        "        sample = expand_dims(scaled_image, 0)\n",
        "        yhat = model.detect(sample, verbose=0)\n",
        "        r = yhat[0]\n",
        "        AP, _, _, _ = compute_ap(gt_bbox, gt_class_id, gt_mask, r['rois'], r['class_ids'], r['scores'], r['masks'])\n",
        "        print(AP)\n",
        "        if math.isnan(AP):\n",
        "            continue\n",
        "        APs.append(AP)\n",
        "    mAP = mean(APs)\n",
        "    return mAP"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKhbt11ebz-C",
        "outputId": "79015eb9-570a-40f9-e6f0-f692ee6ff3b3"
      },
      "source": [
        "\n",
        "training_ds = tongueDataset()\n",
        "training_ds.load_dataset(ds_path, is_train=True)\n",
        "training_ds.prepare()\n",
        "print('Train: %d' % len(training_ds.image_ids))\n",
        "\n",
        "test_ds = tongueDataset()\n",
        "test_ds.load_dataset(ds_path, is_train=False)\n",
        "test_ds.prepare()\n",
        "print('Test: %d' % len(test_ds.image_ids))\n",
        "\n",
        "\n",
        "weights_file = '/content/drive/MyDrive/backup/tongueDiagnosis/models/mask_rcnn_tongue_cfg_0005.h5'  # !!!!!need to identify\n",
        "print(model_path)\n",
        "cfg_pred = predictionConfig()\n",
        "model = MaskRCNN(mode='inference', model_dir=model_path, config=cfg_pred)\n",
        "model.load_weights(weights_file, by_name=True)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: 276\n",
            "Test: 69\n",
            "/content/drive/MyDrive/backup/tongueDiagnosis/models\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GP4ZPDMycUW-",
        "outputId": "30482ef3-e27e-4b7c-a4fe-2f7287c55c4f"
      },
      "source": [
        "train_mAP = evaluate_model(test_ds, model, cfg_pred)\n",
        "print(\"Train mAP: %.3f\" % train_mAP)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'id': '111', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/111.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/111.xml'}\n",
            "1.0\n",
            "{'id': '109', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/109.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/109.xml'}\n",
            "1.0\n",
            "{'id': '110', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/110.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/110.xml'}\n",
            "1.0\n",
            "{'id': '108', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/108.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/108.xml'}\n",
            "1.0\n",
            "{'id': '107', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/107.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/107.xml'}\n",
            "1.0\n",
            "{'id': '113', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/113.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/113.xml'}\n",
            "1.0\n",
            "{'id': '115', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/115.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/115.xml'}\n",
            "1.0\n",
            "{'id': '112', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/112.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/112.xml'}\n",
            "1.0\n",
            "{'id': '116', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/116.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/116.xml'}\n",
            "1.0\n",
            "{'id': '118', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/118.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/118.xml'}\n",
            "1.0\n",
            "{'id': '114', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/114.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/114.xml'}\n",
            "1.0\n",
            "{'id': '117', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/117.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/117.xml'}\n",
            "1.0\n",
            "{'id': '120', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/120.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/120.xml'}\n",
            "1.0\n",
            "{'id': '123', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/123.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/123.xml'}\n",
            "1.0\n",
            "{'id': '124', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/124.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/124.xml'}\n",
            "1.0\n",
            "{'id': '119', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/119.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/119.xml'}\n",
            "1.0\n",
            "{'id': '122', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/122.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/122.xml'}\n",
            "1.0\n",
            "{'id': '121', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/121.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/121.xml'}\n",
            "1.0\n",
            "{'id': '127', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/127.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/127.xml'}\n",
            "1.0\n",
            "{'id': '129', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/129.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/129.xml'}\n",
            "1.0\n",
            "{'id': '128', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/128.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/128.xml'}\n",
            "1.0\n",
            "{'id': '125', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/125.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/125.xml'}\n",
            "1.0\n",
            "{'id': '126', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/126.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/126.xml'}\n",
            "1.0\n",
            "{'id': '135', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/135.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/135.xml'}\n",
            "1.0\n",
            "{'id': '131', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/131.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/131.xml'}\n",
            "1.0\n",
            "{'id': '134', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/134.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/134.xml'}\n",
            "1.0\n",
            "{'id': '132', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/132.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/132.xml'}\n",
            "1.0\n",
            "{'id': '133', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/133.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/133.xml'}\n",
            "1.0\n",
            "{'id': '136', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/136.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/136.xml'}\n",
            "1.0\n",
            "{'id': '140', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/140.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/140.xml'}\n",
            "1.0\n",
            "{'id': '142', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/142.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/142.xml'}\n",
            "1.0\n",
            "{'id': '139', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/139.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/139.xml'}\n",
            "1.0\n",
            "{'id': '138', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/138.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/138.xml'}\n",
            "1.0\n",
            "{'id': '143', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/143.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/143.xml'}\n",
            "1.0\n",
            "{'id': '137', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/137.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/137.xml'}\n",
            "1.0\n",
            "{'id': '150', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/150.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/150.xml'}\n",
            "1.0\n",
            "{'id': '147', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/147.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/147.xml'}\n",
            "1.0\n",
            "{'id': '145', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/145.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/145.xml'}\n",
            "1.0\n",
            "{'id': '144', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/144.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/144.xml'}\n",
            "1.0\n",
            "{'id': '149', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/149.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/149.xml'}\n",
            "1.0\n",
            "{'id': '146', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/146.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/146.xml'}\n",
            "1.0\n",
            "{'id': '148', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/148.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/148.xml'}\n",
            "1.0\n",
            "{'id': '154', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/154.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/154.xml'}\n",
            "1.0\n",
            "{'id': '151', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/151.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/151.xml'}\n",
            "1.0\n",
            "{'id': '153', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/153.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/153.xml'}\n",
            "0.0\n",
            "{'id': '156', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/156.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/156.xml'}\n",
            "1.0\n",
            "{'id': '155', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/155.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/155.xml'}\n",
            "1.0\n",
            "{'id': '152', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/152.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/152.xml'}\n",
            "1.0\n",
            "{'id': '159', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/159.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/159.xml'}\n",
            "1.0\n",
            "{'id': '157', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/157.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/157.xml'}\n",
            "1.0\n",
            "{'id': '160', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/160.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/160.xml'}\n",
            "1.0\n",
            "{'id': '163', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/163.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/163.xml'}\n",
            "1.0\n",
            "{'id': '161', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/161.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/161.xml'}\n",
            "1.0\n",
            "{'id': '158', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/158.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/158.xml'}\n",
            "1.0\n",
            "{'id': '162', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/162.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/162.xml'}\n",
            "1.0\n",
            "{'id': '168', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/168.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/168.xml'}\n",
            "1.0\n",
            "{'id': '166', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/166.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/166.xml'}\n",
            "1.0\n",
            "{'id': '164', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/164.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/164.xml'}\n",
            "1.0\n",
            "{'id': '169', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/169.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/169.xml'}\n",
            "1.0\n",
            "{'id': '170', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/170.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/170.xml'}\n",
            "1.0\n",
            "{'id': '165', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/165.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/165.xml'}\n",
            "1.0\n",
            "{'id': '167', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/167.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/167.xml'}\n",
            "1.0\n",
            "{'id': '172', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/172.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/172.xml'}\n",
            "1.0\n",
            "{'id': '174', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/174.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/174.xml'}\n",
            "1.0\n",
            "{'id': '176', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/176.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/176.xml'}\n",
            "1.0\n",
            "{'id': '175', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/175.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/175.xml'}\n",
            "1.0\n",
            "{'id': '171', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/171.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/171.xml'}\n",
            "1.0\n",
            "{'id': '173', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/173.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/173.xml'}\n",
            "1.0\n",
            "{'id': '177', 'source': 'dataset', 'path': PosixPath('/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/177.jpg'), 'annotation': '/content/drive/MyDrive/backup/tongueDiagnosis/segTraining/annots/177.xml'}\n",
            "1.0\n",
            "Train mAP: 0.986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VL86eAOyfEVV"
      },
      "source": [
        "def plot_actual_vs_predicted(ds, model, cfg, n_image=2):\n",
        "    for i in range(n_image):\n",
        "        image = ds.load_image(i)\n",
        "        mask, _ = ds.load_mask(i)\n",
        "        scaled_image = mold_image(image, cfg)\n",
        "        sample = expand_dims(scaled_image, 0)\n",
        "        yhat = model.detect(sample, verbose=0)[0]\n",
        "        plt.subplot(n_image, 2, i*2+1)\n",
        "        plt.imshow(image)\n",
        "        plt.title('Actual')\n",
        "        for j in range(mask.shape[2]):\n",
        "            plt.imshow(mask[:, :, j], cmap='gray', alpha=0.3)\n",
        "        plt.subplot(n_image, 2, i*2+2)\n",
        "        plt.imshow(image)\n",
        "        plt.title('Predict')\n",
        "        ax = plt.gca()\n",
        "        for box in yhat['rois']:\n",
        "            y1, x1, y2, x2 = box\n",
        "            width, height = x2 - x1, y2 - y1\n",
        "            rect = Rectangle((x1, y1), width, height, fill=False, color='red')\n",
        "            ax.add_patch(rect)\n",
        "    plt.show()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EL3n1imGfT-4"
      },
      "source": [
        "from matplotlib.patches import Rectangle\n",
        "plot_actual_vs_predicted(test_ds, model, cfg_pred, 5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}